{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": []
   },
   "source": [
    "# Wildfire Analysis\n",
    "\n",
    "In this Jupyter Notebook we present an analysis of brazilian wildfires.\n",
    "\n",
    "First, we import all needed libraries and declare some defined values."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import random\n",
    "import os\n",
    "from os import listdir\n",
    "\n",
    "import seaborn as sns\n",
    "\n",
    "import pyspark\n",
    "from pyspark.context import SparkContext\n",
    "from pyspark.sql import SparkSession, SQLContext\n",
    "from pyspark.sql.types import *\n",
    "from pyspark.sql.window import Window\n",
    "import pyspark.sql.functions as F\n",
    "\n",
    "from ipywidgets import interact, widgets\n",
    "\n",
    "DATA_PATH = './data'\n",
    "DATA_FILE = './data.zip'\n",
    "KUDU_MASTER = 'kudu-master-1:7051'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Importing the `Kudu-Spark` connector from Cloudera."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "os.environ['PYSPARK_SUBMIT_ARGS'] = f'--packages org.apache.kudu:kudu-spark3_2.12:1.13.0.7.1.5.17-1 --repositories https://repository.cloudera.com/artifactory/cloudera-repos/ pyspark-shell'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Initializing *Spark*."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING: An illegal reflective access operation has occurred\n",
      "WARNING: Illegal reflective access by org.apache.spark.unsafe.Platform (file:/usr/local/spark-3.2.0-bin-hadoop3.2/jars/spark-unsafe_2.12-3.2.0.jar) to constructor java.nio.DirectByteBuffer(long,int)\n",
      "WARNING: Please consider reporting this to the maintainers of org.apache.spark.unsafe.Platform\n",
      "WARNING: Use --illegal-access=warn to enable warnings of further illegal reflective access operations\n",
      "WARNING: All illegal access operations will be denied in a future release\n",
      "https://repository.cloudera.com/artifactory/cloudera-repos/ added as a remote repository with the name: repo-1\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      ":: loading settings :: url = jar:file:/usr/local/spark-3.2.0-bin-hadoop3.2/jars/ivy-2.5.0.jar!/org/apache/ivy/core/settings/ivysettings.xml\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Ivy Default Cache set to: /home/jovyan/.ivy2/cache\n",
      "The jars for the packages stored in: /home/jovyan/.ivy2/jars\n",
      "org.apache.kudu#kudu-spark3_2.12 added as a dependency\n",
      ":: resolving dependencies :: org.apache.spark#spark-submit-parent-ba54f106-369d-424a-b314-cf2aa187bf2c;1.0\n",
      "\tconfs: [default]\n",
      "\tfound org.apache.kudu#kudu-spark3_2.12;1.13.0.7.1.5.17-1 in repo-1\n",
      ":: resolution report :: resolve 247ms :: artifacts dl 8ms\n",
      "\t:: modules in use:\n",
      "\torg.apache.kudu#kudu-spark3_2.12;1.13.0.7.1.5.17-1 from repo-1 in [default]\n",
      "\t---------------------------------------------------------------------\n",
      "\t|                  |            modules            ||   artifacts   |\n",
      "\t|       conf       | number| search|dwnlded|evicted|| number|dwnlded|\n",
      "\t---------------------------------------------------------------------\n",
      "\t|      default     |   1   |   0   |   0   |   0   ||   1   |   0   |\n",
      "\t---------------------------------------------------------------------\n",
      ":: retrieving :: org.apache.spark#spark-submit-parent-ba54f106-369d-424a-b314-cf2aa187bf2c\n",
      "\tconfs: [default]\n",
      "\t0 artifacts copied, 1 already retrieved (0kB/11ms)\n",
      "21/11/17 01:32:03 WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable\n",
      "Using Spark's default log4j profile: org/apache/spark/log4j-defaults.properties\n",
      "Setting default log level to \"WARN\".\n",
      "To adjust logging level use sc.setLogLevel(newLevel). For SparkR, use setLogLevel(newLevel).\n"
     ]
    }
   ],
   "source": [
    "spark = SparkSession.builder.config('spark.packages', 'org.apache.kudu:kudu-spark3_2.12:1.13.0.7.1.5.17-1').getOrCreate()\n",
    "sc = SparkContext.getOrCreate()\n",
    "sc.setLogLevel('OFF')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Importing the Kudu datastore table."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def read_from_table(name):\n",
    "    table = spark.read.option('kudu.master', KUDU_MASTER).option('kudu.table', f'impala::default.{name}').format('kudu').load()\n",
    "    table.createOrReplaceTempView(name)\n",
    "\n",
    "    return table"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def write_to_table(df, name):\n",
    "    df.write.option('kudu.master', KUDU_MASTER).option('kudu.table', f'impala::default.{name}').mode('append').format('kudu').save()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def persist_df(df, name):\n",
    "    write_to_table(df, name)\n",
    "\n",
    "    return read_from_table(name)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Importing the data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Archive:  ./data.zip\n"
     ]
    }
   ],
   "source": [
    "!unzip -n {DATA_FILE} -d {DATA_PATH}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "loading data files from ./data.zip\n",
      "... Focos_2010-01-01_2010-12-31.csv\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "... Focos_2011-01-01_2011-12-31.csv\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "... Focos_2012-01-01_2012-12-31.csv\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "... Focos_2013-01-01_2013-12-31.csv\n",
      "... Focos_2014-01-01_2014-12-31.csv\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "... Focos_2015-01-01_2015-12-31.csv\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "... Focos_2016-01-01_2016-12-31.csv\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "... Focos_2017-01-01_2017-12-31.csv\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "... Focos_2018-01-01_2018-12-31.csv\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "... Focos_2019-01-01_2019-12-31.csv\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "... Focos_2020-01-01_2020-12-31.csv\n",
      "done\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "data = None\n",
    "\n",
    "print(f'loading data files from {DATA_FILE}')\n",
    "\n",
    "for file in listdir(DATA_PATH):\n",
    "    if not file.endswith('.csv'):\n",
    "        continue\n",
    "\n",
    "    print(f'... {file}')\n",
    "    tmp = spark.read.csv(f'{DATA_PATH}/{file}', header='true', inferSchema='true')\n",
    "    data = data.union(tmp) if data else tmp\n",
    "\n",
    "print('done')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Printing the original table schema"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- datahora: string (nullable = true)\n",
      " |-- satelite: string (nullable = true)\n",
      " |-- pais: string (nullable = true)\n",
      " |-- estado: string (nullable = true)\n",
      " |-- municipio: string (nullable = true)\n",
      " |-- bioma: string (nullable = true)\n",
      " |-- diasemchuva: string (nullable = true)\n",
      " |-- precipitacao: string (nullable = true)\n",
      " |-- riscofogo: string (nullable = true)\n",
      " |-- latitude: double (nullable = true)\n",
      " |-- longitude: double (nullable = true)\n",
      " |-- frp: string (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "data.printSchema()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Changing columns to decimal type."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = data.withColumn('diasemchuva', data.diasemchuva.cast(DecimalType(10, 5))) \\\n",
    "    .withColumn('precipitacao', data.precipitacao.cast(DecimalType(10, 5))) \\\n",
    "    .withColumn('riscofogo', data.riscofogo.cast(DecimalType(10, 5))) \\\n",
    "    .withColumn('latitude', data.latitude.cast(DecimalType(8, 5))) \\\n",
    "    .withColumn('longitude', data.longitude.cast(DecimalType(8, 5))) \\\n",
    "    .withColumn('frp', data.frp.cast(DecimalType(10, 5)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "[T] Separar o campo *datahora* em mês e ano;"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "datahora = data.withColumn('datahora_', F.split(data.datahora, ' '))\n",
    "data = datahora.withColumn('data', F.split(F.element_at(datahora.datahora_, 1), '/'))\n",
    "ano_mes = data.withColumn('ano', F.element_at(data.data, 1).cast(IntegerType())) \\\n",
    "    .withColumn('mes', F.element_at(data.data, 2).cast(IntegerType()))\n",
    "\n",
    "data = ano_mes.drop('datahora_').drop('data')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Printing the resulting table schema"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- datahora: string (nullable = true)\n",
      " |-- satelite: string (nullable = true)\n",
      " |-- pais: string (nullable = true)\n",
      " |-- estado: string (nullable = true)\n",
      " |-- municipio: string (nullable = true)\n",
      " |-- bioma: string (nullable = true)\n",
      " |-- diasemchuva: decimal(10,5) (nullable = true)\n",
      " |-- precipitacao: decimal(10,5) (nullable = true)\n",
      " |-- riscofogo: decimal(10,5) (nullable = true)\n",
      " |-- latitude: decimal(8,5) (nullable = true)\n",
      " |-- longitude: decimal(8,5) (nullable = true)\n",
      " |-- frp: decimal(10,5) (nullable = true)\n",
      " |-- ano: integer (nullable = true)\n",
      " |-- mes: integer (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "data.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "data = persist_df(data, 'queimada')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "[T] Calcular quantas queimadas ocorreram em cada bioma por mês;"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "queimadas_bioma_mes = persist_df(\n",
    "    data.groupBy('bioma', 'ano', 'mes').count(),\n",
    "    'queimadas_bioma_mes'\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "[T] Encontrar os maiores FRP por mês em cada estado a partir de 2018;"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "frp_estado_mes = persist_df(\n",
    "    data[data.ano >= 2018].groupBy('estado', 'ano', 'mes').agg(F.max('frp')).withColumnRenamed('max(frp)', 'max_frp'),\n",
    "    'frp_estado_mes'\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "[C] Retornar o ranking dos biomas em que mais houve focos de queimadas em um determinado mês de um ano;"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------+----+---+-----+\n",
      "|         bioma| ano|mes|count|\n",
      "+--------------+----+---+-----+\n",
      "|      Amazonia|2018|  1| 1444|\n",
      "|       Cerrado|2018|  1|  521|\n",
      "|      Caatinga|2018|  1|  316|\n",
      "|Mata Atlantica|2018|  1|  209|\n",
      "|         Pampa|2018|  1|   40|\n",
      "|      Pantanal|2018|  1|   23|\n",
      "+--------------+----+---+-----+\n",
      "\n",
      "CPU times: user 6.39 ms, sys: 310 µs, total: 6.7 ms\n",
      "Wall time: 431 ms\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "queimadas_bioma_mes[(queimadas_bioma_mes.mes == 1) & (queimadas_bioma_mes.ano == 2018)].orderBy(F.col('count').desc()).show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "[C] Retornar o ranking dos meses em que mais ocorreram queimadas por bioma;"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "def f_queimadas_bioma_mes_rank(x):\n",
    "    w = Window.partitionBy('bioma').orderBy(F.col('count').desc())\n",
    "    queimadas_bioma_mes_rank = queimadas_bioma_mes.withColumn('rank', F.row_number().over(w))\n",
    "    return queimadas_bioma_mes_rank[queimadas_bioma_mes_rank.rank <= x].show(truncate=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------+----+---+-----+----+\n",
      "|bioma   |ano |mes|count|rank|\n",
      "+--------+----+---+-----+----+\n",
      "|Amazonia|2010|8  |45018|1   |\n",
      "|Amazonia|2010|9  |43933|2   |\n",
      "|Amazonia|2017|9  |36569|3   |\n",
      "|Amazonia|2020|9  |32017|4   |\n",
      "|Amazonia|2019|8  |30900|5   |\n",
      "|Amazonia|2015|9  |29326|6   |\n",
      "|Amazonia|2020|8  |29307|7   |\n",
      "|Amazonia|2018|9  |24803|8   |\n",
      "|Amazonia|2012|9  |24067|9   |\n",
      "|Amazonia|2017|8  |21244|10  |\n",
      "|Amazonia|2012|8  |20687|11  |\n",
      "|Amazonia|2014|9  |20522|12  |\n",
      "|Amazonia|2015|8  |20471|13  |\n",
      "|Amazonia|2016|9  |20460|14  |\n",
      "|Amazonia|2014|8  |20113|15  |\n",
      "|Amazonia|2019|9  |19925|16  |\n",
      "|Amazonia|2015|10 |19469|17  |\n",
      "|Amazonia|2016|8  |18340|18  |\n",
      "|Amazonia|2020|10 |17326|19  |\n",
      "|Amazonia|2011|9  |16987|20  |\n",
      "+--------+----+---+-----+----+\n",
      "only showing top 20 rows\n",
      "\n",
      "CPU times: user 6.82 ms, sys: 2.88 ms, total: 9.69 ms\n",
      "Wall time: 858 ms\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "f_queimadas_bioma_mes_rank(20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "95e1b8264e8f424a8ecd442ae1a3987b",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "interactive(children=(IntSlider(value=5, continuous_update=False, description='yes', max=30, min=1), Output())…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "@interact(x=widgets.IntSlider(min=1, max=30, step=1, value=5, description='yes', continuous_update=False))\n",
    "def g(x):\n",
    "    return f_queimadas_bioma_mes_rank(x)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "[C] Calcular o ranking da média mensal de queimadas por bioma a partir do ano *x*;"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "def f_queimadas_bioma_mes(x):\n",
    "    return queimadas_bioma_mes[queimadas_bioma_mes.ano >= x].groupBy('bioma').agg(F.mean('count')).orderBy(F.col('avg(count)').desc()).show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------+------------------+\n",
      "|         bioma|        avg(count)|\n",
      "+--------------+------------------+\n",
      "|      Amazonia| 7447.568181818182|\n",
      "|       Cerrado| 5783.295454545455|\n",
      "|Mata Atlantica|1323.0681818181818|\n",
      "|      Caatinga|1173.9848484848485|\n",
      "|      Pantanal| 554.6060606060606|\n",
      "|         Pampa| 85.52272727272727|\n",
      "+--------------+------------------+\n",
      "\n",
      "CPU times: user 3.35 ms, sys: 5.46 ms, total: 8.8 ms\n",
      "Wall time: 731 ms\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "f_queimadas_bioma_mes(2010)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "d0cf207daaf44360a75864bd4de70a1b",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "interactive(children=(IntSlider(value=2015, continuous_update=False, description='yes', max=2020, min=2010), O…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "@interact(x=widgets.IntSlider(min=2010, max=2020, step=1, value=2015, description='yes', continuous_update=False))\n",
    "def g(x):\n",
    "    return f_queimadas_bioma_mes(x)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "[C] Retornar o ranking dos maiores focos com maiores FRP por estado em um determinado mês de um ano;"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------------------+----+---+----------+\n",
      "|             estado| ano|mes|   max_frp|\n",
      "+-------------------+----+---+----------+\n",
      "|            RORAIMA|2018|  1|1018.00000|\n",
      "|  RIO GRANDE DO SUL|2018|  1| 404.70000|\n",
      "|               PARA|2018|  1| 291.30000|\n",
      "|            ALAGOAS|2018|  1| 283.70000|\n",
      "|              BAHIA|2018|  1| 264.10000|\n",
      "|           MARANHAO|2018|  1| 227.50000|\n",
      "|          TOCANTINS|2018|  1| 226.40000|\n",
      "|            PARAIBA|2018|  1| 220.60000|\n",
      "|        MATO GROSSO|2018|  1| 213.50000|\n",
      "|              PIAUI|2018|  1| 199.00000|\n",
      "|              CEARA|2018|  1| 169.40000|\n",
      "|         PERNAMBUCO|2018|  1| 115.00000|\n",
      "|RIO GRANDE DO NORTE|2018|  1| 104.90000|\n",
      "|              AMAPA|2018|  1|  96.90000|\n",
      "|       MINAS GERAIS|2018|  1|  92.00000|\n",
      "| MATO GROSSO DO SUL|2018|  1|  84.30000|\n",
      "|              GOIAS|2018|  1|  83.10000|\n",
      "|            SERGIPE|2018|  1|  81.40000|\n",
      "|           AMAZONAS|2018|  1|  72.00000|\n",
      "|           RONDONIA|2018|  1|  58.90000|\n",
      "+-------------------+----+---+----------+\n",
      "only showing top 20 rows\n",
      "\n",
      "CPU times: user 6.56 ms, sys: 1.38 ms, total: 7.94 ms\n",
      "Wall time: 501 ms\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "frp_estado_mes[(frp_estado_mes.mes == 1) & (frp_estado_mes.ano == 2018)].orderBy(F.col('max_frp').desc()).show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "#sc.stop()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
